{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XY8ocPbnCpiA"
   },
   "source": [
    "#  Automated Book Publication Workflow\n",
    "\n",
    "This project demonstrates a semi-automated publishing workflow using AI and human collaboration. It features:\n",
    "- Web scraping and screenshots from Wikisource\n",
    "- AI-based chapter spinning using Hugging Face\n",
    "- Human-in-the-loop editing and review\n",
    "- Content versioning with ChromaDB\n",
    "- Intelligent search with reinforcement learning-inspired scoring\n",
    "\n",
    "Developed in Python, fully runnable in Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install playwright\n",
    "!playwright install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nest_asyncio\n",
    "!pip install beautifulsoup4 chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Image, display\n",
    "\n",
    "URL = \"https://en.wikisource.org/wiki/The_Gates_of_Morning/Book_1/Chapter_1\"\n",
    "screenshot_path = \"/content/chapter1.png\"\n",
    "\n",
    "async def fetch_and_screenshot():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(URL)\n",
    "        await page.screenshot(path=screenshot_path, full_page=True)\n",
    "        content = await page.content()\n",
    "        await browser.close()\n",
    "        return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = await fetch_and_screenshot()\n",
    "display(Image(filename=screenshot_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Wikisource main content is typically inside <div id=\"mw-content-text\">\n",
    "content_div = soup.find(\"div\", id=\"mw-content-text\")\n",
    "\n",
    "# Clean and join all paragraphs\n",
    "paragraphs = content_div.find_all(\"p\")\n",
    "chapter_text = \"\\n\\n\".join([para.get_text(strip=True) for para in paragraphs if para.get_text(strip=True)])\n",
    "\n",
    "print(\"ðŸ“– Extracted Chapter Text:\\n\")\n",
    "print(chapter_text[:1000])  # Preview the first 1000 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maEcej5RC9tF"
   },
   "source": [
    "##  AI Writer â€“ Chapter Spinning\n",
    "\n",
    "The original chapter is rephrased (\"spun\") using a Hugging Face Transformer (`gpt2`) to simulate an AI writing assistant. This is the first draft that will later go through human and AI review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"Vamsi/T5_Paraphrase_Paws\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spun_version = spin_chapter(chapter_text)\n",
    "print(\"ðŸŒ€ Spun Chapter:\\n\")\n",
    "print(spun_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAHALjzrDLr-"
   },
   "source": [
    "##  Versioning via ChromaDB\n",
    "\n",
    "Each chapter version (original, spun, reviewed) is stored in ChromaDB along with metadata. This allows:\n",
    "- Easy version tracking\n",
    "- Metadata-based search and retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Use the new default way (no need for Settings)\n",
    "client = chromadb.PersistentClient(path=\"/content/chromadb\")\n",
    "\n",
    "collection = client.get_or_create_collection(\"gates_of_morning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[chapter_text, spun_version],\n",
    "    metadatas=[\n",
    "        {\"version\": \"v1\", \"role\": \"original\", \"notes\": \"Raw HTML scrape\"},\n",
    "        {\"version\": \"v2\", \"role\": \"AI Writer\", \"notes\": \"Spun with HF T5 model\"},\n",
    "    ],\n",
    "    ids=[\"chapter1_v1\", \"chapter1_v2\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_revision = spun_version.replace(\"He\", \"The man\")  # Dummy edit\n",
    "\n",
    "collection.add(\n",
    "    documents=[human_revision],\n",
    "    metadatas=[{\"version\": \"v3\", \"role\": \"Human Editor\", \"notes\": \"Reviewed and adjusted\"}],\n",
    "    ids=[\"chapter1_v3\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.get(ids=[\"chapter1_v1\", \"chapter1_v2\", \"chapter1_v3\"])\n",
    "for doc, meta in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "    print(f\"ðŸ“˜ {meta['version']} ({meta['role']}):\\n{doc[:1500]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UovKW_CxDY_f"
   },
   "source": [
    "##  Intelligent Retrieval via RL-style Scoring\n",
    "\n",
    "Simulated reinforcement learning rewards are applied to rank the chapter versions. Final selection is based on:\n",
    "- Embedding similarity to the query\n",
    "- Reward score (e.g., human feedback or editor preference)\n",
    "\n",
    "This helps find the \"best\" version dynamically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Embed all versions\n",
    "embeddings = model.encode([chapter_text, spun_version])\n",
    "\n",
    "# Add embedded documents (if not already added)\n",
    "collection.upsert(\n",
    "    ids=[\"chapter1_v1\", \"chapter1_v2\"],\n",
    "    embeddings=embeddings,\n",
    "    documents=[chapter_text, spun_version],\n",
    "    metadatas=[\n",
    "        {\"version\": \"v1\", \"role\": \"original\"},\n",
    "        {\"version\": \"v2\", \"role\": \"AI Writer\"},\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated feedback scores\n",
    "reward_scores = {\n",
    "    \"chapter1_v1\": 0.4,  # Original\n",
    "    \"chapter1_v2\": 0.85, # AI Writer output\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"human-like, literary rewrite of the chapter\"\n",
    "\n",
    "query_vec = model.encode([query])[0]\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_vec],\n",
    "    n_results=2,\n",
    "    include=[\"metadatas\", \"documents\", \"distances\"]\n",
    ")\n",
    "\n",
    "# Combine similarity + reward\n",
    "for i, doc_id in enumerate(results[\"ids\"][0]):\n",
    "    score = (1 - results[\"distances\"][0][i]) * 0.5 + reward_scores[doc_id] * 0.5\n",
    "    print(f\"ðŸ“„ Doc ID: {doc_id} | Final Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
